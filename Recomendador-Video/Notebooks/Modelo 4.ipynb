{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 4\n",
    "\n",
    "Vamos aplicar os modelos de Machine Learning para os dados relacionados ao videos do Youtube. É importante ressaltar que foram revistos 0 e 1 aplicados a coluna 'y' para que tenha menos carga aleatória. Além disso, foi preenchido manualmente todas as informações de acordo com o título."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('raw_data_with_label_rev1.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dos dados\n",
    "\n",
    "Nesta etapa vamos retirar linhas repetidas a onde um mesmo video aparece para duas palavras-chaves (ex: Machine Learning e Kaggle).\n",
    "\n",
    "Além disso, vamos verificar os valores ausentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletando as linhas duplicadas\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "y              0\n",
       "upload_date    2\n",
       "view_count     2\n",
       "query          2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando a quantidade de nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletando as linhas que estão com valores ausentes\n",
    "df.dropna(axis = 0, subset = ['upload_date', 'view_count', 'query'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertendo a variável date para datetime\n",
    "df['upload_date'] = pd.to_datetime(df['upload_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Esta etapa está associada a criação de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armazenando os dados com y preenchido\n",
    "df_preenchido = df.loc[ df['y'].notnull() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando um dataframe vazio para features com o mesmo índices do df_preenchido\n",
    "features = pd.DataFrame(index = df_preenchido.index)\n",
    "\n",
    "#Criando um dataframe exclusivo para y -> Variável Alvo\n",
    "y = df_preenchido['y'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Criando uma feature de data desde da publicação**\n",
    "\n",
    "Como os modelos de Machine Learning não conseguem ler datas, devemos criar alguma forma para transformar estas datas em dados numéricos. Portanto, iremos fazer a diferença em relação a data de hoje (31/05/2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5 days\n",
       "1        5 days\n",
       "2        5 days\n",
       "3        5 days\n",
       "4        5 days\n",
       "         ...   \n",
       "1505   915 days\n",
       "1506   938 days\n",
       "1507   942 days\n",
       "1508   950 days\n",
       "1509   951 days\n",
       "Name: upload_date, Length: 1507, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime('2021/05/31')-df_preenchido['upload_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar vem com a palavras \"days\" e no formato timedelta64. Para contornarmos isto, utilizaremos um método do numpy chamado **timedelta64** em que passaremos como argumento a diferença de um dia, já que os dados estão associados a diferença diária. Isto permite que os valores sejam convertidos para float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         5.0\n",
       "1         5.0\n",
       "2         5.0\n",
       "3         5.0\n",
       "4         5.0\n",
       "        ...  \n",
       "1505    915.0\n",
       "1506    938.0\n",
       "1507    942.0\n",
       "1508    950.0\n",
       "1509    951.0\n",
       "Name: upload_date, Length: 1507, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.to_datetime('2021/05/31')-df_preenchido['upload_date'])/np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a feature de diferença diária\n",
    "features['tempo_desde_pub'] = (pd.to_datetime('2021/05/31')-df_preenchido['upload_date'])/np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features - Views**\n",
    "Criando um conjunto de features para view.\n",
    "\n",
    "Iremos replicar o count_views e iremos calcular as views por dia, utilizando a feature tempo_desde_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armazenando as views\n",
    "features['views'] = df_preenchido['view_count']\n",
    "\n",
    "#Fazendo o cálculo de views por dia\n",
    "features['views_por_dia'] = features['views'] / features['tempo_desde_pub'] # Serve para avaliar a capacidade do video em promover views\n",
    "                                                                            # E desconsiderar o fator tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é uma séria temporal e os dados devem ser fatiados considerando a sequência temporal, a feature **tempo_desde_pub** vai trazer a mesma informação do que a variável **date**, ou seja, se fatiássemos em 01/01/2020, os valores antes desta data seriam menores e após seriam maiores, porque seguem a mesma sequência temporal.\n",
    "\n",
    "Diante disto, vamos deletar esta coluna do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletando a variável tempo_desde_pub\n",
    "features.drop(['tempo_desde_pub'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views</th>\n",
       "      <th>views_por_dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.0</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   views  views_por_dia\n",
       "0    7.0            1.4\n",
       "1   27.0            5.4\n",
       "2    1.0            0.2\n",
       "3   20.0            4.0\n",
       "4    2.0            0.4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imprimindo as 5 primeiras linhas\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515, 2), (992, 2), (515,), (992,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separando entre treino e teste\n",
    "mask_train = df_preenchido['upload_date'] < \"2020-04-03\" #Utilizado +- a metade do período temporal\n",
    "mask_val = df_preenchido['upload_date'] >= \"2020-04-03\"\n",
    "\n",
    "X_train, X_val = features[mask_train], features[mask_val]\n",
    "y_train, y_val = y[mask_train], y[mask_val]\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o método **TfidfVectorizer** para extrair palavras do título. Este método é bom, porque ele atribui maior peso as palavras que aparecem menos.\n",
    "\n",
    "Além disso, utilizamos este método para transformar informações textuais em números.\n",
    "\n",
    "Utilizaremos o argumento min_df = 2 para que seja considerada como coluna apenas as palavras que aparecerem 2 vezes ou mais no dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o método\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Criando uma variável de títulos com os dados de treino e validação\n",
    "title_train = df_preenchido.loc[mask_train, 'title']\n",
    "title_val = df_preenchido.loc[mask_val, 'title']\n",
    "\n",
    "#Instanciando o objeto TfidfVectorizer\n",
    "title_vec = TfidfVectorizer(min_df = 2, ngram_range = (1,1))\n",
    "\n",
    "#Treinando e transformando os dados de treino referente ao título\n",
    "title_bow_train = title_vec.fit_transform(title_train)\n",
    "\n",
    "#Transformando os dados de validação com os parâmetros de treino\n",
    "title_bow_val = title_vec.transform(title_val)\n",
    "\n",
    "#o termo bow significa bag of words -> devido a matriz de palavras geradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados gerados após aplicação do método é uma matriz esparsa que possui a seguinte definição:\n",
    "* Quando possui matrizes em que existem muitos zeros, a matriz esparsa apenas armazena (na memória) os valores diferentes de zero. Portanto, quando ela for consultar um elemento e ele não estiver armazenado, retorna zero.\n",
    "\n",
    "O próximo passo é unirmos os dados de título com o X_train e X_val. Para isto, utilizaremos um método do Scipy chamado hstack e funciona da seguinte maneira:\n",
    "\n",
    "*Mostraremos o funcionamento também da vstack*\n",
    "\n",
    "hstack - [1 2]     [3 4]   -> [1 2 3 4] - 1x4 -> Junta os elementos em colunas \n",
    "\n",
    "vstack - [1 2]     [3 4]   -> [1 2]\n",
    "                              [3 4] - 2x2 -> Junta os elementos em linha\n",
    "                              \n",
    "Não utilizar estes métodos do numpy, apenas do scipy sparse, porque o numpy não está otimizado para trabalhar com matrizes esparsas, com isso, o tempo de aplicação do método seria maior do que scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando o método hstack\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "#Aplicando o método hstack para juntar o X com as matrizes de bag of word\n",
    "X_train_wtitle = hstack([X_train, title_bow_train])\n",
    "X_val_wtitle = hstack([X_val, title_bow_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((515, 433), (992, 433))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imprimindo as dimensões\n",
    "X_train_wtitle.shape, X_val_wtitle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', n_estimators=1000, n_jobs=6,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instanciando a RandomForest\n",
    "#Como os dados estão desbalanceados, vamos utilizar o argumento class_weight para dar mais peso a classe desbalanceada\n",
    "mdl = RandomForestClassifier(n_estimators=1000, random_state=0, min_samples_leaf = 1, class_weight=\"balanced\", n_jobs=6)\n",
    "mdl.fit(X_train_wtitle, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pegando as probabilidade\n",
    "p = mdl.predict_proba(X_val_wtitle)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5758634597636195"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculando a metrica average_precision_score\n",
    "average_precision_score(y_val, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489579818847162"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculando a ROC_AUC\n",
    "roc_auc_score(y_val, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tunar os o parâmetros **TfidfVectorizer** e **Random Forest** para verificar se as métricas melhoram ou não. Neste caso, vamos apenas modificar os valores dos códigos acima e registrar os valores obtidos nas métricas.\n",
    "\n",
    "Para **TfidfVectorizer** simularemos os seguintes parâmetros:\n",
    "* min_df -> A frequência mínima que a palavra deve ter para contabilizar como coluna\n",
    "* ngram_range -> Representado por uma tupla em que a primeira posição está associado ao mínimo de gramas e a última ao máximo de gramas. Segue um exemplo para ilustras: intro to machine learning -> intro, to, machine, learning -> ngram_range=(1,1) \n",
    "                       intro to machine learning -> intro, to, machine, learning, intro to, to machine, machine learning -> ngram_range=(1,2)\n",
    "                       intro to machine learning -> intro to, to machine, machine learning -> ngram_range=(2,2)\n",
    "                       \n",
    "É importante destacar que o ngrama é uma palavra.\n",
    "\n",
    "Para o **Random Forest** usaremos:\n",
    "* n_estimators: Número de árvores -> 100 ou 1000\n",
    "* min_samples_leaf: Mostra quantos dados devem ter nos nós finais das árvores.\n",
    "\n",
    "Resultado:\n",
    "* Padrão (min_df = 2, n_estimators = 1000) -> ROC_AUC = 0.7551062780618204 e AP = 0.581860377926327\n",
    "* Parâmetros (min_df = 2, n_estimators = 1000, min_sample_lif = 2) -> ROC_AUC = 0.7678285017258831 e AP = 0.575313410802137\n",
    "* Parâmetros (min_df = 2, n_estimators = 100, min_sample_lif = 2) -> ROC_AUC = 0.7614621992681218 e AP = 0.5736100883158195\n",
    "* Parâmetros (min_df = 2, n_estimators = 100, min_sample_lif = 1) -> ROC_AUC = 0.7370870208403624 e AP = 0.5614679067140127\n",
    "* Parâmetro (min_df = 1, n_estimators = 1000) -> ROC_AUC = 0.7530404090213075 e AP = 0.5588239160420265\n",
    "* Parâmetro (min_df = 3, n_estimators = 1000) -> ROC_AUC = 0.7489579818847162 e AP = 0.5758634597636195\n",
    "* Parâmetro (min_df = 2, n_estimators = 1000, ngram (1,2)) -> ROC_AUC = 0.7385455866704731 e AP = 0.5768589581963872\n",
    "* Parâmetro (min_df = 2, n_estimators = 1000, ngram (1,3)) -> ROC_AUC = 0.7212478264254756 e AP = 0.5635050362865264\n",
    "* Parâmetro (min_df = 2, n_estimators = 1000, ngram (1,4)) -> ROC_AUC = 0.7201370325192702 e AP = 0.5612889724473165\n",
    "\n",
    "Apesar do ROC_AUC melhorar um pouco nas mudanças de parâmetros, vamos continuar com os parâmetros iniciais, porque conseguimos manter uma boa average_precision_recall\n",
    "\n",
    "* Padrão (min_df = 2, n_estimators = 1000) -> ROC_AUC = 0.7551062780618204 e AP = 0.581860377926327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LigthGBM\n",
    "\n",
    "Parâmetros importantes:\n",
    "* Learning_rate -> Basicamente significa a contribuição de cada árvore para combinação de árvore que o modelo vai criar. Neste parâmetro estamos mais preocupados em buscar valores menores.\n",
    "* num_leaves -> Número máximo de folhas, ou seja, de nós finais. Quanto maior for este número, mais complexo será o modelo e é uma forma de limitar o modelo de não memorizar os dados e aprender os padrões que achemos necessário. Este parâmetro não é a mesma coisa do que profundidade, porque nem sempre a árvore vai ser balanceada, ou seja, o lado direito e esquerdo terminarão no mesmo nível de profundidade.\n",
    "* min_child_sample -> Número mínimo de amostras por nó. Este é um outro parâmetro que ajuda a evitar que o modelo memorize os dados\n",
    "* subsample -> Cada vez que for criar uma árvore não vai ser sob todos os dados, este parâmetro serve para definir as amostras que serão criadas para cada árvore. Ajuda a dar diversidade a árvore e evitar a memorização dos dados. Devemos sempre preencher a subsample_freq que significa a quantas árvores devemos amostrar os dados.\n",
    "* colsample_bytree -> É uma forma de não deixar todas as variáveis disponíveis para todas as árvores. É similar ao subsample em que é disponibilizado uma amostra de variáveis para cada árvore. Ajuda também a dar diversidade a árvore e evitar a memorização dos dados.\n",
    "\n",
    "Formas de tunar os hiperparâmetros, utilizando a biblioteca scikit-optimize:\n",
    "* dummy_minimize -> É preciso criar uma função (def) para treinar o modelo a partir dos parâmetros escolhidos. Devemos passar para o dummy_minimize esta função, quantidade de iterações e uma lista dos parâmetros em que cada parâmetros deve ser informado através de uma tupla com o valor mínimo e máximo para variação. Podemos também informar a distribuição para gerar pesos diferentes, por default, é uniforme. A ideia desta função é rodar de forma aleatória várias combinações de parâmetros até que minimize uma função 'custo' que deve ser definida como retorno na função criada inicialmente. Vale destacar que os scores utilizados são na maioria quanto maior melhor, por isso, para estes casos devemos utilizar, por exemplo, - roc_auc_score.\n",
    "\n",
    "* bayesiana_optimizacion (gp_minimize - utiliza processos gaussianos e forest_minimize - utiliza random forest. A escolha depende da quantidade de parâmetros para tunar ) -> Ela também roda de forma aleatória, porém, é mais guiada. Com isso, avalia o impacto do score quando mudamos os parâmetros dentro do modelo escolhido. Então, ela avalia a sensibilidade das mudanças destes parâmetros no modelo escolhido, priorizando um caminho de maior impacto para maximizar o score. Portanto, alguns parâmetros serão considerados mais importantes do que outros. O problema da utilização do GridSearch que se perde muito tempo avaliando parâmetros de pouco impacto sem uma busca mais inteligente, já que considera que todos parâmetros possuem mesmo impacto. Os argumentos de entrada são as mesmas da dummy_minimize, porém, deve acrescentar n_random_starts (geralmente é 10), basicamente serve para pegar 10 pontos aleatórios para avaliar a combinação de parâmetros melhor e ir guiando os próximos passos, tentando avaliar os parâmetros mais promissores (avaliar uma região de pontos e qual é o ponto ótimo. Diante disso, passa para o próximos 10 pontos e avaliar o próximo ponto ótimo). \n",
    "\n",
    "* Comentário do Mario Filho - Ele é a favor de testar tudo, porém, não devemos aplicar o GridSearch e o modelo Naive Bayes (Modelo bem fraquinho). Existe papers e artigos mostrando que a Random Search chega em uma resultado muito melhor em menos tempo do que a GridSearch. Além disso, alguns artigos mostram que bayesina optimization possui resultados iguais ou melhores que a Random Search. Lembrando que a Bayesiana Optimization funciona como fosse uma busca aleatória, porém, guiada pela sensibilidade do modelo em relação as range de parâmetros.\n",
    "\n",
    "**IMPORTANTE** \n",
    "* Utilizar a forest_minimize quando tivermos mais de 5 parâmetros para tunar\n",
    "* Menos do que este valor, podemos utilizar a gp_minimize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-optimize in c:\\users\\fabio\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: pyaml>=16.9 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-optimize) (20.4.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-optimize) (0.17.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-optimize) (0.23.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (5.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->scikit-optimize) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "#Instalando scikit-optimize\n",
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\fabio\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from lightgbm) (1.5.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\fabio\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "#Instalando lighgbm\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicação do LighGBM sem tunar os hiperparâmetros**\n",
    "\n",
    "Primeiro vamos avaliar a performance do modelo sem a tunar os hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP = 0.4851778075837254\n",
      "ROC_AUC = 0.7070903947470868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "#Importando o modelo\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#Instanciando o modelo\n",
    "mdl_lgm = LGBMClassifier(random_state = 42, class_weight=\"balanced\", n_jobs=6)\n",
    "\n",
    "#Treinando o modelo\n",
    "mdl_lgm.fit(X_train_wtitle, y_train)\n",
    "\n",
    "#Pegando as probabilidade\n",
    "p = mdl_lgm.predict_proba(X_val_wtitle)[:,1]\n",
    "\n",
    "#Calculando a metrica average_precision_score\n",
    "print(\"AP = {}\".format(average_precision_score(y_val, p)))\n",
    "\n",
    "#Calculando a ROC_AUC\n",
    "print(\"ROC_AUC = {}\".format(roc_auc_score(y_val, p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tunando os hiperparâmetros**\n",
    "\n",
    "Iremos tunar tanto os hiperparâmetros do LighGBM quanto TfidfVectorizer,como fizemos na Random Forest.\n",
    "\n",
    "O próximo passo é criarmos uma função que receba os parâmetros e busque minimizar o erro. Escolheremos como métrica principal a **average_precision**.\n",
    "\n",
    "É importante destacar que os parâmetros são fornecidos a função aleatoriamente, seguindo uma distribuição uniforme. Porém, para o caso específico da **learning_rate**, utilizaremos uma distribuição log-uniforme para gerar um peso maior para os valores menores. Como vimos anteriormente, quanto menor for este valor melhor será.\n",
    "\n",
    "Os parâmetros devem ser dados em forma de tupla, contendo o valor mínimo e máximo para variação, podendo também informar uma distribuição diferente da uniforme. Vale destacar que se dermos os parâmetros sem ponto, a variação será feita de forma inteira e com ponto de forma decimal. Exemeplo:\n",
    "(100, 1000) -> Irá variar de forma inteira -> 100, 897, 465 e etc\n",
    "(0.1, 1.) -> Irá variar de forma decimal -> 0.2, 0.78 e etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a função\n",
    "def tune_lgbm(params):\n",
    "    \n",
    "    #Imprimindo os parâmetros\n",
    "    print(params)\n",
    "    \n",
    "    #Atribuindo os paramêtros LGBM\n",
    "    lr = params[0]\n",
    "    max_depth = params[1]\n",
    "    min_child_samples = params[2]\n",
    "    subsample = params[3]\n",
    "    colsample_bytree = params[4]\n",
    "    n_estimators = params[5]\n",
    "    \n",
    "    #Atribuindo os parâmetros para bag of words\n",
    "    min_df = params[6]\n",
    "    ngram_range = (1, params[7]) #Considerando como valor padrão mínimo a unigrama\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------Criando a Bag of Words com os parâmetros tunados-------------------------------\n",
    "    \n",
    "    #Instanciando o objeto TfidfVectorizer\n",
    "    title_vec = TfidfVectorizer(min_df = min_df, ngram_range = ngram_range)\n",
    "\n",
    "    #Treinando e transformando os dados de treino referente ao título\n",
    "    title_bow_train = title_vec.fit_transform(title_train)\n",
    "\n",
    "    #Transformando os dados de validação com os parâmetros de treino\n",
    "    title_bow_val = title_vec.transform(title_val)\n",
    "    \n",
    "    #----------------------------------------------Juntando o X com as bag of words -------------------------------------------\n",
    "    \n",
    "    #Aplicando o método hstack para juntar o X com as matrizes de bag of word\n",
    "    X_train_wtitle = hstack([X_train, title_bow_train])\n",
    "    X_val_wtitle = hstack([X_val, title_bow_val])\n",
    "    \n",
    "    #---------------------------------------------Tunando os parâmetros e aplicando-------------------------------------------\n",
    "    \n",
    "    #Instanciando o modelo\n",
    "    mdl_lgm = LGBMClassifier(random_state = 42, \n",
    "                             class_weight=\"balanced\", \n",
    "                             n_jobs=6, \n",
    "                             learning_rate = lr,\n",
    "                             num_leaves = 2 ** max_depth, #Para dar um sentido hipotético do tamanho da árvore, olhar comentário do Mario\n",
    "                             max_depth = max_depth,\n",
    "                             min_child_samples = min_child_samples,\n",
    "                             subsample = subsample,\n",
    "                             colsample_bytree = colsample_bytree,\n",
    "                             n_estimators = n_estimators,\n",
    "                             bagging_freq = 1)\n",
    "\n",
    "    #Treinando o modelo\n",
    "    mdl_lgm.fit(X_train_wtitle, y_train)\n",
    "\n",
    "    #Pegando as probabilidade\n",
    "    p = mdl_lgm.predict_proba(X_val_wtitle)[:,1]\n",
    "\n",
    "    #Calculando a ROC_AUC\n",
    "    print(\"ROC_AUC = {}\".format(roc_auc_score(y_val, p)))\n",
    "    \n",
    "    #Retorno da função custo -> Como queremos minimizar, devemos atribuir o sinal de \"-\" a frente do score para funcionar\n",
    "    return -average_precision_score(y_val, p)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[0.03918194347141743, 8, 7, 0.4735411152109116, 0.14497617002710275, 558, 5, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7192208870779373\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.2393\n",
      "Function value obtained: -0.5215\n",
      "Current minimum: -0.5215\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "[0.0019307837536547132, 3, 2, 0.7358988336534836, 0.9416250735649627, 485, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7528691183722196\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1788\n",
      "Function value obtained: -0.6059\n",
      "Current minimum: -0.6059\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "[0.017177621112338382, 10, 12, 0.07190930378934497, 0.5485359272454697, 610, 2, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6220238249721004\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.2224\n",
      "Function value obtained: -0.4460\n",
      "Current minimum: -0.6059\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "[0.08861577452533079, 3, 12, 0.538522716492931, 0.6127938404189405, 230, 5, 3]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7184319119669876\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0816\n",
      "Function value obtained: -0.5153\n",
      "Current minimum: -0.6059\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "[0.022941144328643994, 9, 7, 0.06260171310187321, 0.9450916679006103, 113, 2, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.6471153097505905\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0671\n",
      "Function value obtained: -0.4338\n",
      "Current minimum: -0.6059\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "[0.004066563313514795, 5, 2, 0.7000213751865492, 0.46814486905262126, 554, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.76312839012743\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.3255\n",
      "Function value obtained: -0.6184\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "[0.04638630972397281, 3, 14, 0.42150757719457876, 0.22312428339865925, 487, 2, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.6872518232072877\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.1312\n",
      "Function value obtained: -0.4951\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "[0.013658426050382541, 2, 12, 0.9425239944859798, 0.9000859829062665, 369, 4, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7733331603124757\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0678\n",
      "Function value obtained: -0.5798\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "[0.08362652463906246, 5, 16, 0.7599541046305119, 0.5627075257696259, 891, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6645558122031611\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.2134\n",
      "Function value obtained: -0.4685\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "[0.0036464395589807202, 9, 15, 0.20700359210985236, 0.06485458640413425, 620, 1, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6205367107004749\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.1975\n",
      "Function value obtained: -0.4077\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "[0.0038634593707206726, 3, 17, 0.7257748551112175, 0.8006667635046455, 478, 5, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7263554021437284\n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.1067\n",
      "Function value obtained: -0.5546\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "[0.005211124595788266, 9, 7, 0.8575366489003095, 0.4769781404312932, 132, 5, 3]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7243881549920843\n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1411\n",
      "Function value obtained: -0.5447\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "[0.015226341829186321, 6, 3, 0.49860417890385184, 0.16361453364138662, 561, 3, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7552386390179336\n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.2483\n",
      "Function value obtained: -0.5599\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "[0.027762530094438326, 7, 14, 0.5191058165461713, 0.5465961879128944, 114, 2, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7102748436323999\n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0798\n",
      "Function value obtained: -0.5224\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "[0.007562632622090494, 3, 7, 0.5016017120230062, 0.5851117933775646, 999, 5, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7784381407178635\n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.2429\n",
      "Function value obtained: -0.6082\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "[0.012013849374287178, 4, 13, 0.12313091433735335, 0.3252638802680796, 185, 4, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.6471801925722146\n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0671\n",
      "Function value obtained: -0.4125\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "[0.01772997389934229, 9, 6, 0.48370784195876476, 0.25751841535599196, 439, 3, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7360904207002155\n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.2351\n",
      "Function value obtained: -0.5548\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "[0.0017545910486566185, 9, 7, 0.9114870194684664, 0.3085256369154036, 382, 1, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7119747735589524\n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.2992\n",
      "Function value obtained: -0.5253\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "[0.005071514980544015, 3, 7, 0.20642306048579467, 0.55738494840667, 417, 5, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7097142560535672\n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1017\n",
      "Function value obtained: -0.5086\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "[0.0017367237151593167, 8, 15, 0.20987649006428405, 0.25782600859441673, 571, 1, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6296774026108847\n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.4079\n",
      "Function value obtained: -0.4206\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "[0.0017013737055127612, 3, 1, 0.2716364760541711, 0.10889703345012566, 840, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7275362694972879\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5340\n",
      "Function value obtained: -0.5387\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "[0.005868851394145974, 6, 2, 0.8370275159600182, 0.7752328285487989, 937, 1, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7411201370325193\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8156\n",
      "Function value obtained: -0.5768\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "[0.004335375212438368, 5, 18, 0.8205841711924284, 0.8274776752791814, 688, 3, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7154914224909812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3889\n",
      "Function value obtained: -0.5254\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "[0.0015584891369652362, 5, 1, 0.5262495922269302, 0.05658538665283824, 784, 1, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7111883937608678\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0964\n",
      "Function value obtained: -0.5012\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "[0.004475844754929862, 7, 3, 0.44341464892457927, 0.8129537930374527, 178, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7489605771975811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4153\n",
      "Function value obtained: -0.5998\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "[0.010723009813690527, 9, 1, 0.20821022118942184, 0.7173786104232759, 472, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7225480781708236\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.7125\n",
      "Function value obtained: -0.5329\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "[0.0028248968565978854, 5, 5, 0.6976211426092485, 0.39779445865762225, 149, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7308426980872543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3441\n",
      "Function value obtained: -0.5816\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "[0.0031145712192530823, 10, 13, 0.7583614147326043, 0.05023376083172441, 508, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.6715657522514339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3971\n",
      "Function value obtained: -0.4808\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "[0.015950081185306934, 5, 2, 0.6809517402379428, 0.9559360207299168, 153, 1, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7480833614492226\n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4666\n",
      "Function value obtained: -0.5922\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "[0.007792842258967527, 1, 6, 0.7140364657431296, 0.6326761404548193, 882, 1, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7517219900859049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3944\n",
      "Function value obtained: -0.5792\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "[0.004324716091054493, 3, 2, 0.05131195953008831, 0.4352232080944541, 560, 1, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6612415976745997\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5374\n",
      "Function value obtained: -0.4659\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "[0.09492532235755752, 1, 1, 0.5521756712315306, 0.584629423192453, 489, 5, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7604863616308946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3732\n",
      "Function value obtained: -0.5800\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "[0.002298394346661213, 3, 7, 0.5016233812654619, 0.5186571843644752, 891, 2, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7382678881939216\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4559\n",
      "Function value obtained: -0.5906\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "[0.011414189760999312, 3, 8, 0.4367367996502325, 0.4988830517632764, 750, 4, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7360047753756715\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4347\n",
      "Function value obtained: -0.5567\n",
      "Current minimum: -0.6184\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "[0.003168092666102539, 5, 1, 0.7431150268827361, 0.9009262002860146, 467, 2, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7571046689678441\n",
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5457\n",
      "Function value obtained: -0.6206\n",
      "Current minimum: -0.6206\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "[0.01636555620463727, 4, 7, 0.36821866891182925, 0.9801314536385267, 902, 2, 5]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.740533596325037\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4767\n",
      "Function value obtained: -0.5738\n",
      "Current minimum: -0.6206\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "[0.0028411831978363278, 10, 1, 0.7159903506544794, 0.9434180682803618, 325, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7668319015857362\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8280\n",
      "Function value obtained: -0.6154\n",
      "Current minimum: -0.6206\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "[0.0013292027737930054, 8, 2, 0.35080976384732376, 0.872171904721914, 359, 3, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7270016350471049\n",
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5507\n",
      "Function value obtained: -0.5746\n",
      "Current minimum: -0.6206\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "[0.0010773419143320184, 2, 2, 0.34879040336652434, 0.9405055067603683, 259, 1, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7056292336041109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3460\n",
      "Function value obtained: -0.5109\n",
      "Current minimum: -0.6206\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "[0.005607394468548006, 7, 1, 0.6655993304551099, 0.9586294865021325, 523, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7966338792141392\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6543\n",
      "Function value obtained: -0.6402\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "[0.04244768043919099, 5, 1, 0.6562945025605821, 0.9406491127481088, 886, 1, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7284757727544056\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.0944\n",
      "Function value obtained: -0.5565\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "[0.005218815195420768, 10, 1, 0.9152653861483127, 0.6905391440661648, 308, 2, 4]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7336508266071474\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0910\n",
      "Function value obtained: -0.5746\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "[0.0011349897209374283, 6, 2, 0.557858898785668, 0.8944367560066796, 541, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.765482338895954\n",
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4930\n",
      "Function value obtained: -0.5976\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "[0.005064842775048966, 9, 4, 0.7637027020220357, 0.9828691716369488, 748, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7814746567698736\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6227\n",
      "Function value obtained: -0.5981\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "[0.06882081676358107, 7, 5, 0.846389900948811, 0.8256407632494277, 493, 3, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7460460408502245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3853\n",
      "Function value obtained: -0.5590\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "[0.0028433716438414237, 3, 1, 0.7026336644367411, 0.44633273920287136, 760, 2, 2]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7619449274610055\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4792\n",
      "Function value obtained: -0.6100\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "[0.0016146479047848103, 7, 1, 0.6639249427479458, 0.9089963318477061, 569, 1, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7604214788092704\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.8900\n",
      "Function value obtained: -0.5934\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "[0.0034725026038687403, 4, 1, 0.6816639893359455, 0.2904332074843246, 677, 3, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7665178687290755\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4288\n",
      "Function value obtained: -0.5924\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "[0.010127818745010784, 9, 1, 0.4765189617330622, 0.9031844020096405, 474, 5, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7658923983286186\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6836\n",
      "Function value obtained: -0.5927\n",
      "Current minimum: -0.6402\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "[0.0022855320267921395, 6, 1, 0.4445021474318012, 0.9804824738416845, 150, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7620150009083595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3472\n",
      "Function value obtained: -0.5971\n",
      "Current minimum: -0.6402\n"
     ]
    }
   ],
   "source": [
    "#Importando o baysian optimize\n",
    "from skopt import forest_minimize\n",
    "\n",
    "#Setando os parâmetros em tuplas\n",
    "space = [(1e-3, 1e-1, 'log-uniform'), # lr - Log-Uniform gera um peso maior para os valores menores, conforme dito anteriormente\n",
    "          (1, 10), # max_depth\n",
    "          (1, 20), # min_child_samples\n",
    "          (0.05, 1.), # subsample\n",
    "          (0.05, 1.), # colsample_bytree\n",
    "          (100,1000), # n_estimators\n",
    "          (1,5), # min_df\n",
    "          (1,5)] # ngram_range\n",
    "\n",
    "#Aplicando o tunning\n",
    "res = forest_minimize(tune_lgbm, space, random_state=42, n_random_starts=20, n_calls=50, verbose=1)\n",
    "# n_random_starts -> Pelo o que entendi, quantos eventos precisa reunir de forma aleatória para avaliar o ponto ótimo\n",
    "# n_calls -> quantidade de vezes que vai ser aplicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.005607394468548006, 7, 1, 0.6655993304551099, 0.9586294865021325, 523, 2, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Melhor hiperparâmetros\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6402330807008632"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Melhor score da average precision\n",
    "res.fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005607394468548006, 7, 1, 0.6655993304551099, 0.9586294865021325, 523, 2, 1]\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "ROC_AUC = 0.7966338792141392\n",
      "Average_precision = 0.6402330807008632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:739: UserWarning: Converting data to scipy sparse matrix.\n",
      "  _log_warning('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "#Vamos replicaro os parâmetros tunados\n",
    "ap = tune_lgbm(res.x)\n",
    "\n",
    "#Imprimindo o valor\n",
    "print(\"Average_precision = {}\".format(-ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora conseguimos os seguintes valores:\n",
    "\n",
    "* LGBM tunado -> ROC_AUC = 0.7966338792141392 e Average_precision = 0.6402330807008632\n",
    "* Random Forest tunado -> ROC_AUC = 0.7551062780618204 e AP = 0.581860377926327\n",
    "\n",
    "Não aplicamos este formato de tunning na Random Forest, porque, segundo o Mario Filho, este modelo são menos sensíveis a mudanças dos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressão Logística\n",
    "\n",
    "Como a função de otimização deste modelo é baseada no **Gradiente Descendente**, será necessário escalonar as variáveis numéricas com o intuito de manter todas as variáveis na mesma escalara. Para isto, utilizaremos dois métodos em busca do mais eficiente:\n",
    "* StandarScaler -> É um dos mais usuais, utilizamos para buscar uma escala com média igual a zero e desvio padrão igual. \n",
    "* MaxAbsScaler -> Divide todos os valores pelo maior valor da coluna -> ex [100,500,1000,50] = [100 / 10000, 500 / 1000, 1000 / 1000, 50 / 1000]\n",
    "\n",
    "Como as matrizes são dadas em matriz esparsas, será necessário aplicar também um método chamado de **csr_matrix** para termos acesso as colunas para o escalonamento das variáveis numéricas.\n",
    "\n",
    "O próximo passo é aplicarmos a Regressão Logística sem tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando as bibliotecas\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:124: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler() - AP = 0.5747056828293885\n",
      "StandardScaler() - ROC_AUC = 0.7719913835612883\n",
      "MaxAbsScaler() - AP = 0.586897637438734\n",
      "MaxAbsScaler() - ROC_AUC = 0.7700033739067244\n"
     ]
    }
   ],
   "source": [
    "#Intânciando os scalers\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_max_abs = MaxAbsScaler()\n",
    "\n",
    "#Fazendo um for para cada scaler\n",
    "for scaler in [scaler_standard, scaler_max_abs]:\n",
    "        \n",
    "    #Criando um cópia do X_train e X_val com csr_matrix para manipulação de variáveis durante o scaler\n",
    "    X_train_wtitle2 = csr_matrix(X_train_wtitle.copy())\n",
    "    X_val_wtitle2 = csr_matrix(X_val_wtitle.copy())\n",
    "    \n",
    "    if scaler == scaler_standard:\n",
    "    \n",
    "        #Escalonando as variáveis numéricas\n",
    "        X_train_wtitle2[: , :2] = scaler.fit_transform(X_train_wtitle2[:, :2].todense()) #Matriz densa é o oposto de matriz esparsa\n",
    "        X_val_wtitle2[: , :2] = scaler.transform(X_val_wtitle2[:, :2].todense())\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        #Escalonando todas as variáveis - Como pega o valor máximo por coluna não gera problema quando for escalonar as variáveis categóricas\n",
    "        X_train_wtitle2 = scaler.fit_transform(X_train_wtitle2.todense()) #Matriz densa é o oposto de matriz esparsa\n",
    "        X_val_wtitle2 = scaler.transform(X_val_wtitle2.todense())\n",
    "    \n",
    "    #Instanciando o modelo\n",
    "    mdl = LogisticRegression(n_jobs = 6, random_state = 42, class_weight=\"balanced\")\n",
    "    \n",
    "    #Treinando o modelo\n",
    "    mdl.fit(X_train_wtitle2, y_train)\n",
    "    \n",
    "    #Prevendo o modelo\n",
    "    p = mdl.predict_proba(X_val_wtitle2)[:, 1]\n",
    "    \n",
    "    #Calculando os scores\n",
    "    print(\"{} - AP = {}\".format(str(scaler), average_precision_score(y_val, p) ))\n",
    "    print(\"{} - ROC_AUC = {}\".format(str(scaler), roc_auc_score(y_val, p) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sem tunning**\n",
    "* StandardScaler() - AP = 0.5747056828293885 e ROC_AUC = 0.7705587708598272\n",
    "* MaxAbsScaler() - AP = 0.586897637438734 e ROC_AUC = 0.7700033739067244\n",
    "\n",
    "O próximo passo é tentarmos tunar os hiperparâmetros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando a função\n",
    "def tune_rgl(params):\n",
    "    \n",
    "    #Imprimindo os parâmetros\n",
    "    print(params)\n",
    "    \n",
    "    #Criando um lista de parâmetros para solver, scaler e penalty\n",
    "    solver_list = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    penalty_list = ['l1', 'l2', 'none']\n",
    "    scaler_list = [StandardScaler(), MaxAbsScaler()]\n",
    "    \n",
    "    #Atribuindo os paramêtros LGBM\n",
    "    c = params[0]\n",
    "    solver = solver_list[ params[1] ]\n",
    "    penalty = penalty_list[ params[2] ]\n",
    "    \n",
    "    #Atribuindo os parâmetros para bag of words - Forçar para ficar na mesma referência dos últimos modelos e ser menos complexo o Deploy, assim evita ter que criar mais de um Vectorizer se a Regressão Logística for escolhida para o ensemble\n",
    "    min_df = 2\n",
    "    ngram_range = (1, 1) #Considerando como valor padrão mínimo a unigrama\n",
    "    \n",
    "    #Atribuindo os parâmetros scaler\n",
    "    scaler = scaler_list[ params[3] ]\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------Criando a Bag of Words com os parâmetros tunados-------------------------------\n",
    "    \n",
    "    #Instanciando o objeto TfidfVectorizer\n",
    "    title_vec = TfidfVectorizer(min_df = min_df, ngram_range = ngram_range)\n",
    "\n",
    "    #Treinando e transformando os dados de treino referente ao título\n",
    "    title_bow_train = title_vec.fit_transform(title_train)\n",
    "\n",
    "    #Transformando os dados de validação com os parâmetros de treino\n",
    "    title_bow_val = title_vec.transform(title_val)\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------Juntando o X com as bag of words -------------------------------------------\n",
    "    \n",
    "    #Aplicando o método hstack para juntar o X com as matrizes de bag of word\n",
    "    X_train_wtitle = hstack([X_train, title_bow_train])\n",
    "    X_val_wtitle = hstack([X_val, title_bow_val])\n",
    "    \n",
    "    #----------------------------------------------Escalonando as variáveis numéricas -----------------------------------------\n",
    "    \n",
    "    #Criando um cópia do X_train e X_val com csr_matrix para manipulação de variáveis durante o scaler\n",
    "    X_train_wtitle2 = csr_matrix(X_train_wtitle.copy())\n",
    "    X_val_wtitle2 = csr_matrix(X_val_wtitle.copy())\n",
    "    \n",
    "    if scaler == StandardScaler():\n",
    "    \n",
    "        #Escalonando as variáveis numéricas\n",
    "        X_train_wtitle2[: , :2] = scaler.fit_transform(X_train_wtitle2[:, :2].todense()) #Matriz densa é o oposto de matriz esparsa\n",
    "        X_val_wtitle2[: , :2] = scaler.transform(X_val_wtitle2[:, :2].todense())\n",
    "        \n",
    "    else :\n",
    "        \n",
    "        #Escalonando todas as variáveis - Como pega o valor máximo por coluna não gera problema quando for escalonar as variáveis categóricas\n",
    "        X_train_wtitle2 = scaler.fit_transform(X_train_wtitle2.todense()) #Matriz densa é o oposto de matriz esparsa\n",
    "        X_val_wtitle2 = scaler.transform(X_val_wtitle2.todense())\n",
    "    \n",
    "    #---------------------------------------------Tunando os parâmetros e aplicando-------------------------------------------\n",
    "    \n",
    "    #Alguns conjunto de parâmetros não funcionam juntos, por isto, vamos criar uma exception\n",
    "    try:\n",
    "    \n",
    "        #Instanciando o modelo\n",
    "        mdl_rlg = LogisticRegression(n_jobs = 6, \n",
    "                                     random_state = 42, \n",
    "                                     class_weight=\"balanced\",\n",
    "                                     C = c,\n",
    "                                     solver = solver,\n",
    "                                     penalty = penalty)\n",
    "\n",
    "        #Treinando o modelo\n",
    "        mdl_rlg.fit(X_train_wtitle2, y_train)\n",
    "\n",
    "        #Pegando as probabilidade\n",
    "        p = mdl_rlg.predict_proba(X_val_wtitle2)[:,1]\n",
    "\n",
    "        #Calculando a ROC_AUC\n",
    "        print(\"ROC_AUC = {}\".format(roc_auc_score(y_val, p)))\n",
    "\n",
    "        #Retorno da função custo -> Como queremos minimizar, devemos atribuir o sinal de \"-\" a frente do score para funcionar\n",
    "        return -average_precision_score(y_val, p)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        #Cria um valor muito ruim para não ser escolhido com ponto ótimo\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[15.35224694197351, 1, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.6945394617321119\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.6290\n",
      "Function value obtained: -0.4107\n",
      "Current minimum: -0.4107\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "[0.6071989493441302, 0, 1, 0]\n",
      "ROC_AUC = 0.7294230619501181\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1720\n",
      "Function value obtained: -0.4939\n",
      "Current minimum: -0.4939\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "[0.03727925903376984, 3, 0, 1]\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.0239\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.4939\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "[56.78201970293135, 0, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.581067192650074\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 1.2943\n",
      "Function value obtained: -0.3089\n",
      "Current minimum: -0.4939\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "[2.796485951606247, 0, 0, 1]\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0259\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.4939\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "[0.3975977214318099, 0, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.5660636889777062\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.9208\n",
      "Function value obtained: -0.3018\n",
      "Current minimum: -0.4939\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "[0.023036990230378637, 2, 1, 1]\n",
      "ROC_AUC = 0.7429083075964807\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0309\n",
      "Function value obtained: -0.5095\n",
      "Current minimum: -0.5095\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "[0.7362945281639222, 3, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7236407048869742\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.2214\n",
      "Function value obtained: -0.4939\n",
      "Current minimum: -0.5095\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "[0.011299516083106616, 4, 1, 0]\n",
      "ROC_AUC = 0.7257896239391659\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.1476\n",
      "Function value obtained: -0.5129\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "[0.011584172310543985, 1, 0, 1]\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0259\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "[2.754143921332031, 3, 0, 0]\n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0289\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "[0.053572800696018295, 3, 1, 0]\n",
      "ROC_AUC = 0.7245179206353325\n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1860\n",
      "Function value obtained: -0.4992\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "[1.8655260217376843, 0, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.5660636889777062\n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.9407\n",
      "Function value obtained: -0.3018\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "[0.38071583792493946, 4, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.72242609846617\n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.2110\n",
      "Function value obtained: -0.4948\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "[1.9132683954526493, 2, 2, 1]\n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0209\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.5129\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "[9.756194291174571, 2, 1, 1]\n",
      "ROC_AUC = 0.7592483973943058\n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0329\n",
      "Function value obtained: -0.5557\n",
      "Current minimum: -0.5557\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "[2.6800135558226996, 1, 1, 0]\n",
      "ROC_AUC = 0.7323869092419092\n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.6113\n",
      "Function value obtained: -0.4949\n",
      "Current minimum: -0.5557\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "[0.011549033124221243, 2, 1, 0]\n",
      "ROC_AUC = 0.7256442864187278\n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.0359\n",
      "Function value obtained: -0.5115\n",
      "Current minimum: -0.5557\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "[0.01138463970530464, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7376242506034103\n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.5895\n",
      "Function value obtained: -0.5000\n",
      "Current minimum: -0.5557\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "[2.6536271329026335, 4, 1, 1]\n",
      "ROC_AUC = 0.7657314889309905\n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.3172\n",
      "Function value obtained: -0.5770\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "[0.01025256660388722, 4, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7535231372141911\n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4299\n",
      "Function value obtained: -0.5368\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "[0.010912872479933167, 4, 0, 1]\n",
      "ROC_AUC = 0.5\n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.2902\n",
      "Function value obtained: -0.2671\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "[98.31967022939952, 2, 0, 0]\n",
      "ROC_AUC = 0.7643455918610988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3875\n",
      "Function value obtained: -0.5228\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "[0.01, 4, 1, 1]\n",
      "ROC_AUC = 0.7370532817731178\n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3496\n",
      "Function value obtained: -0.4984\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "[0.010699653419954169, 3, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7218655108873375\n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6682\n",
      "Function value obtained: -0.4921\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "[99.99997230222202, 4, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7535231372141911\n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6908\n",
      "Function value obtained: -0.5368\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "[0.01, 2, 0, 0]\n",
      "ROC_AUC = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6213\n",
      "Function value obtained: -0.2671\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "[100.0, 4, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7541200591731333\n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6973\n",
      "Function value obtained: -0.5382\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "[100.0, 2, 1, 0]\n",
      "ROC_AUC = 0.7375775349718409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4159\n",
      "Function value obtained: -0.5097\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "[100.0, 4, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7541200591731333\n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6488\n",
      "Function value obtained: -0.5382\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 31 started. Searching for the next optimal point.\n",
      "[100.0, 3, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7514365056707586\n",
      "Iteration No: 31 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6468\n",
      "Function value obtained: -0.5310\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 32 started. Searching for the next optimal point.\n",
      "[99.99999769385104, 0, 1, 1]\n",
      "ROC_AUC = 0.7517064182087152\n",
      "Iteration No: 32 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6836\n",
      "Function value obtained: -0.5282\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 33 started. Searching for the next optimal point.\n",
      "[0.01, 3, 1, 0]\n",
      "ROC_AUC = 0.7257325270561366\n",
      "Iteration No: 33 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4318\n",
      "Function value obtained: -0.5142\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 34 started. Searching for the next optimal point.\n",
      "[99.99999432345643, 4, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.7214450702032131\n",
      "Iteration No: 34 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5755\n",
      "Function value obtained: -0.4943\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 35 started. Searching for the next optimal point.\n",
      "[23.488648549549538, 2, 1, 1]\n",
      "ROC_AUC = 0.7553969531026965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 35 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4713\n",
      "Function value obtained: -0.5404\n",
      "Current minimum: -0.5770\n",
      "Iteration No: 36 started. Searching for the next optimal point.\n",
      "[2.611232469951016, 4, 1, 1]\n",
      "ROC_AUC = 0.7658093483169396\n",
      "Iteration No: 36 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6114\n",
      "Function value obtained: -0.5773\n",
      "Current minimum: -0.5773\n",
      "Iteration No: 37 started. Searching for the next optimal point.\n",
      "[0.010963040023495452, 3, 1, 1]\n",
      "ROC_AUC = 0.7374633412057823\n",
      "Iteration No: 37 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5655\n",
      "Function value obtained: -0.4996\n",
      "Current minimum: -0.5773\n",
      "Iteration No: 38 started. Searching for the next optimal point.\n",
      "[1.325803322893768, 2, 1, 0]\n",
      "ROC_AUC = 0.7326360592769459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 38 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4274\n",
      "Function value obtained: -0.4977\n",
      "Current minimum: -0.5773\n",
      "Iteration No: 39 started. Searching for the next optimal point.\n",
      "[2.607323809216881, 4, 1, 1]\n",
      "ROC_AUC = 0.7658249201941294\n",
      "Iteration No: 39 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5206\n",
      "Function value obtained: -0.5773\n",
      "Current minimum: -0.5773\n",
      "Iteration No: 40 started. Searching for the next optimal point.\n",
      "[1.9364484092835832, 3, 1, 1]\n",
      "ROC_AUC = 0.7675014923048973\n",
      "Iteration No: 40 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4508\n",
      "Function value obtained: -0.5818\n",
      "Current minimum: -0.5818\n",
      "Iteration No: 41 started. Searching for the next optimal point.\n",
      "[0.01, 0, 2, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.581067192650074\n",
      "Iteration No: 41 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.2332\n",
      "Function value obtained: -0.3089\n",
      "Current minimum: -0.5818\n",
      "Iteration No: 42 started. Searching for the next optimal point.\n",
      "[0.9638851701068524, 4, 1, 1]\n",
      "ROC_AUC = 0.7701279489242429\n",
      "Iteration No: 42 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5336\n",
      "Function value obtained: -0.5877\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 43 started. Searching for the next optimal point.\n",
      "[0.5888341100374204, 2, 1, 1]\n",
      "ROC_AUC = 0.7701383301757027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1353: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 6.\n",
      "  warnings.warn(\"'n_jobs' > 1 does not have any effect when\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 43 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.3949\n",
      "Function value obtained: -0.5857\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 44 started. Searching for the next optimal point.\n",
      "[1.1775685394292426, 1, 1, 1]\n",
      "ROC_AUC = 0.7695673613454102\n",
      "Iteration No: 44 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0339\n",
      "Function value obtained: -0.5859\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 45 started. Searching for the next optimal point.\n",
      "[0.3920267184345894, 0, 1, 1]\n",
      "ROC_AUC = 0.7701227582985128\n",
      "Iteration No: 45 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6258\n",
      "Function value obtained: -0.5822\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 46 started. Searching for the next optimal point.\n",
      "[100.0, 4, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabio\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC = 0.756741325166749\n",
      "Iteration No: 46 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.7711\n",
      "Function value obtained: -0.5397\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 47 started. Searching for the next optimal point.\n",
      "[0.34305161564276565, 4, 1, 1]\n",
      "ROC_AUC = 0.7701487114271626\n",
      "Iteration No: 47 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.6662\n",
      "Function value obtained: -0.5812\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 48 started. Searching for the next optimal point.\n",
      "[1.103880368995756, 3, 1, 1]\n",
      "ROC_AUC = 0.769956658275155\n",
      "Iteration No: 48 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.4498\n",
      "Function value obtained: -0.5867\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 49 started. Searching for the next optimal point.\n",
      "[100.0, 0, 0, 1]\n",
      "Iteration No: 49 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.5949\n",
      "Function value obtained: 0.0000\n",
      "Current minimum: -0.5877\n",
      "Iteration No: 50 started. Searching for the next optimal point.\n",
      "[7.6715142938224625, 1, 1, 1]\n",
      "ROC_AUC = 0.7601152318912046\n",
      "Iteration No: 50 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0303\n",
      "Function value obtained: -0.5592\n",
      "Current minimum: -0.5877\n"
     ]
    }
   ],
   "source": [
    "#Importando o baysian optimize\n",
    "from skopt import gp_minimize\n",
    "\n",
    "#Setando os parâmetros em tuplas\n",
    "space = [(1e-2, 100., 'log-uniform'), # C\n",
    "          (0, 4), # solver\n",
    "          (0, 2), # penalty\n",
    "          (0,1)] #scaler \n",
    "\n",
    "#Aplicando o tunning\n",
    "res = gp_minimize(tune_rgl, space, random_state=42, n_random_starts=20, n_calls=50, verbose=1)\n",
    "# n_random_starts -> Pelo o que entendi, quantos eventos precisa reunir de forma aleatória para avaliar o ponto ótimo\n",
    "# n_calls -> quantidade de vezes que vai ser aplicado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9638851701068524, 4, 1, 1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imprimindo os melhores parâmetros\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9638851701068524, 4, 1, 1]\n",
      "ROC_AUC = 0.7701279489242429\n",
      "Average_precision = 0.5877359562456916\n"
     ]
    }
   ],
   "source": [
    "#Vamos replicar os parâmetros tunados\n",
    "ap = tune_rgl(res.x)\n",
    "\n",
    "#Imprimindo o valor\n",
    "print(\"Average_precision = {}\".format(-ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O tunning melhorou muito pouco em relação ao sem tunning.\n",
    "\n",
    "* Regressão Logística -> AP = 0.5877359562456916 e ROC_AUC = 0.7701279489242429\n",
    "* LGBM tunado -> ROC_AUC = 0.7966338792141392 e Average_precision = 0.6402330807008632\n",
    "* Random Forest tunado -> ROC_AUC = 0.7551062780618204 e AP = 0.581860377926327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
